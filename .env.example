# Starting block height for node synchronization (0 = start from the beginning)
# START_HEIGHT=0

# Stop block height for node synchronization (Infinity = keep syncing until stopped)
# STOP_HEIGHT="Infinity"

# Arweave node to use for fetching data
# TRUSTED_NODE_URL="https://arweave.net"

# Arweave node to use for proxying requests
# TRUSTED_GATEWAY_URL="https://arweave.net"

# Trusted gateways configuration (JSON object with URL keys and priority values)
# TRUSTED_GATEWAYS_URLS='{"https://arweave.net": 1, "https://ar-io.dev": 2}'

# Origins to block when forwarding to trusted gateways (JSON array)
# TRUSTED_GATEWAYS_BLOCKED_ORIGINS='["self.gateway.com", "loop.source.net"]'

# IP addresses/CIDR ranges to block when forwarding to trusted gateways (JSON array)
# TRUSTED_GATEWAYS_BLOCKED_IP_ADDRESSES='["192.168.1.100", "10.0.0.0/8"]'

# ArNS gateway
# TRUSTED_ARNS_GATEWAY_URL="https://__NAME__.arweave.dev"

# If true, skips the local cache and always fetches headers from the node
# SKIP_CACHE=false

# If true, skips the data cache (read-through data cache) and always fetches data from upstream
# SKIP_DATA_CACHE=false

# Adds an "INSTANCE_ID" field to output logs
# INSTANCE_ID=""

# Sets the format of output logs, accepts "simple" and "json"
# LOG_FORMAT="simple"

# AR.IO node exposed port number
# CORE_PORT=4000
# ENVOY_PORT=3000
# CLICKHOUSE_PORT=9000
# CLICKHOUSE_PORT_2=8123
# CLICKHOUSE_PORT_3=8443
# OBSERVER_PORT=5050

# Number from 0 to 1, representing the probability of a request failing
# SIMULATED_REQUEST_FAILURE_RATE=0

# Arweave wallet address used for staking and rewards
# AR_IO_WALLET=""

# Arweave wallet address used by the observer to upload report data and interact with the AR.IO process
# OBSERVER_WALLET=""

# Admin key value used for accessing the admin API
# ADMIN_API_KEY="secret"

# Alternatively use filepath to admin key used for accessing the admin API
# it takes precedence over ADMIN_API_KEY
# ADMIN_API_KEY_FILE="/path/to/admin-key.txt"

# If true, ar.io node will start indexing missing bundles
# BACKFILL_BUNDLE_RECORDS=false

# If true, all indexed bundles will be reprocessed with the new filters (you can use this when you change the filters)
# FILTER_CHANGE_REPROCESS=false

# Only bundles compliant with this filter will be unbundled
# ANS104_UNBUNDLE_FILTER={"never": true}

# Only bundles compliant with this filter will be indexed
# ANS104_INDEX_FILTER={"never": true}

# Root host for ArNS
# ARNS_ROOT_HOST=""

# Protocol setting in process of creating sandbox domain in ArNS (ARNS_ROOT_HOST needs to be set for this env to have any effect)
# SANDBOX_PROTOCOL=""

# If true, start indexing blocks, tx, ANS104 bundles
# START_WRITERS=true

# If true, the observer runs when using docker-compose
# RUN_OBSERVER=false

# Sets target server for webhooks, webhooks disabled if not set
# WEBHOOK_TARGET_SERVERS="http://localhost:3000,http://localhost:3001"

# Sets the webhook index filter, webhooks disabled if both index and block filters are not set
# WEBHOOK_INDEX_FILTER={"never": true}

# Sets the webhook block filter, webhooks disabled if both index and block filters are not set
# WEBHOOK_BLOCK_FILTER={"never": true}

# If true, the observer will start indexing pending tx from the mempool
# ENABLE_MEMPOOL_WATCHER=true

# Sets the mempool polling interval in milliseconds
# MEMPOOL_POLLING_INTERVAL_MS=30000

# Data source retrieval order for on-demand data requests (comma-separated list of sources)
# Available sources: 'trusted-gateways', 'ar-io-network', 'chunks-offset-aware', 'chunks-data-item' (deprecated), 'turbo', 'tx-data'
# ON_DEMAND_RETRIEVAL_ORDER="trusted-gateways,ar-io-network,chunks-offset-aware,tx-data"

# Chunk data retrieval order (comma-separated list of sources)
# Available sources: 'arweave-network', 'legacy-s3'
# CHUNK_DATA_RETRIEVAL_ORDER="arweave-network"

# Chunk metadata retrieval order (comma-separated list of sources)
# Available sources: 'arweave-network', 'legacy-psql'
# CHUNK_METADATA_RETRIEVAL_ORDER="arweave-network"

# Parallelism settings for chunk source operations
# CHUNK_DATA_SOURCE_PARALLELISM=1
# CHUNK_METADATA_SOURCE_PARALLELISM=1

# Preferred chunk GET nodes (comma-separated URLs)
# Defaults to http://data-1.arweave.xyz:1984 through http://data-12.arweave.xyz:1984 if not set
# To override defaults, provide your own comma-separated list of URLs
# PREFERRED_CHUNK_GET_NODE_URLS=http://custom1.example.com:1984,http://custom2.example.com:1984

# Preferred chunk POST nodes (comma-separated URLs)
# Defaults to http://tip-2.arweave.xyz:1984 through http://tip-4.arweave.xyz:1984 if not set
# To override defaults, provide your own comma-separated list of URLs
# PREFERRED_CHUNK_POST_NODE_URLS=http://custom1.example.com:1984,http://custom2.example.com:1984

# DNS resolution interval for preferred chunk nodes (in seconds)
# DNS resolution reduces lookup overhead by resolving hostnames to IPs on startup
# Applies to both GET and POST nodes
# Set to 0 to disable periodic re-resolution (default: 3600 - 1 hour)
# PREFERRED_CHUNK_NODE_DNS_RESOLUTION_INTERVAL_SECONDS=3600

# ==============================================================================
# ClickHouse Auto-Import Configuration (optional)
# ==============================================================================

# Sleep interval between import cycles (seconds, default: 3600)
# CLICKHOUSE_AUTO_IMPORT_SLEEP_INTERVAL=3600

# Height range to process in each batch (default: 10000)  
# CLICKHOUSE_AUTO_IMPORT_HEIGHT_INTERVAL=10000

# Maximum rows per Parquet file (default: 1000000)
# CLICKHOUSE_AUTO_IMPORT_MAX_ROWS_PER_FILE=1000000

# Enable Apache Iceberg metadata generation (default: false)
# ENABLE_ICEBERG_GENERATION=false

# ==============================================================================
# Datasets Configuration
# ==============================================================================

# Enable datasets endpoint at /local/datasets (default: false)
# ENABLE_DATASETS_ENDPOINT=true

# Datasets proxy configuration (optional)
# By default, /local/datasets routes to the core service
# Set these to proxy to an external datasets service
# DATASETS_PROXY_HOST=datasets.example.com
# DATASETS_PROXY_PORT=8080
