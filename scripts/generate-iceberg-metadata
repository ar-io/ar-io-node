#!/usr/bin/env python3
"""
AR.IO Gateway - Generate Apache Iceberg Metadata for Parquet Exports
Creates PyIceberg table metadata matching DuckDB's exported Parquet schema
"""

import os
import sys
import glob
import argparse
from pathlib import Path

try:
    from pyiceberg.catalog import load_catalog
    from pyiceberg.catalog.sql import SqlCatalog
    from pyiceberg.schema import Schema
    from pyiceberg.types import (
        NestedField, StringType, LongType, IntegerType, 
        BooleanType, BinaryType, DoubleType, DecimalType
    )
    from pyiceberg.partitioning import PartitionSpec, PartitionField
    from pyiceberg.transforms import IdentityTransform
    from pyiceberg.table import Table
    from pyiceberg import expressions
    import pyarrow.parquet as pq
    import pyarrow as pa
    import pyarrow.dataset as ds
    import pandas as pd
except ImportError as e:
    print(f"Error: Required library not installed: {e}")
    print("\nPlease install PyIceberg and dependencies:")
    print("  pip install 'pyiceberg[pyarrow,duckdb,sql]'")
    sys.exit(1)


# Define schemas that match EXACTLY what DuckDB exports
def get_blocks_schema():
    """Get the schema for blocks table matching DuckDB export"""
    return Schema(
        NestedField(1, "indep_hash", BinaryType(), required=False),
        NestedField(2, "height", LongType(), required=False),  # uint64 -> long
        NestedField(3, "previous_block", BinaryType(), required=False),
        NestedField(4, "nonce", BinaryType(), required=False),
        NestedField(5, "hash", BinaryType(), required=False),
        NestedField(6, "block_timestamp", IntegerType(), required=False),  # int32
        NestedField(7, "tx_count", IntegerType(), required=False),  # int32
        NestedField(8, "block_size", LongType(), required=False),  # uint64 -> long
    )


def get_transactions_schema():
    """Get the schema for transactions table matching DuckDB export"""
    return Schema(
        NestedField(1, "id", BinaryType(), required=False),
        NestedField(2, "indexed_at", LongType(), required=False),  # uint64 -> long
        NestedField(3, "block_transaction_index", IntegerType(), required=False),  # uint16 -> int
        NestedField(4, "is_data_item", BooleanType(), required=False),  # bool
        NestedField(5, "target", BinaryType(), required=False),
        NestedField(6, "quantity", DecimalType(20, 0), required=False),  # decimal128(20,0)
        NestedField(7, "reward", DecimalType(20, 0), required=False),  # decimal128(20,0)
        NestedField(8, "anchor", BinaryType(), required=False),
        NestedField(9, "data_size", LongType(), required=False),  # uint64 -> long
        NestedField(10, "content_type", StringType(), required=False),  # string
        NestedField(11, "format", IntegerType(), required=False),  # uint8 -> int
        NestedField(12, "height", LongType(), required=False),  # uint64 -> long
        NestedField(13, "owner_address", BinaryType(), required=False),
        NestedField(14, "data_root", BinaryType(), required=False),
        NestedField(15, "parent", BinaryType(), required=False),
        NestedField(16, "offset", LongType(), required=False),  # uint64 -> long
        NestedField(17, "size", LongType(), required=False),  # uint64 -> long
        NestedField(18, "data_offset", LongType(), required=False),  # uint64 -> long
        NestedField(19, "owner_offset", LongType(), required=False),  # uint64 -> long
        NestedField(20, "owner_size", IntegerType(), required=False),  # uint32 -> int
        NestedField(21, "owner", BinaryType(), required=False),
        NestedField(22, "signature_offset", LongType(), required=False),  # uint64 -> long
        NestedField(23, "signature_size", IntegerType(), required=False),  # uint32 -> int
        NestedField(24, "signature_type", IntegerType(), required=False),  # uint32 -> int
        NestedField(25, "root_transaction_id", BinaryType(), required=False),
        NestedField(26, "root_parent_offset", IntegerType(), required=False),  # uint32 -> int
    )


def get_tags_schema():
    """Get the schema for tags table matching DuckDB export"""
    return Schema(
        NestedField(1, "height", LongType(), required=False),  # uint64 -> long
        NestedField(2, "id", BinaryType(), required=False),
        NestedField(3, "tag_index", IntegerType(), required=False),  # uint16 -> int
        NestedField(4, "indexed_at", LongType(), required=False),  # uint64 -> long
        NestedField(5, "tag_name", BinaryType(), required=False),
        NestedField(6, "tag_value", BinaryType(), required=False),
        NestedField(7, "is_data_item", BooleanType(), required=False),  # bool
    )


def create_local_catalog(warehouse_dir, warehouse_root=None):
    """Create a local file-based Iceberg catalog using SQL catalog
    
    Args:
        warehouse_dir: Local directory path for the warehouse
        warehouse_root: Optional custom warehouse URI for metadata references.
                       This sets where the metadata claims the data files are located.
                       For remote URIs (https://, s3://), the catalog still writes locally,
                       but metadata will reference the remote location.
    """
    catalog_path = os.path.join(warehouse_dir, "catalog.db")
    
    # Determine warehouse URI for metadata
    if warehouse_root:
        # Custom warehouse root - this is what will be written to metadata
        warehouse_uri = warehouse_root.rstrip('/')
        
        # Special handling for certain cases
        if warehouse_uri in ('file:', 'file://'):
            # Just file:// without path - use absolute local path
            warehouse_uri = f"file://{os.path.abspath(warehouse_dir)}"
        elif not any(warehouse_uri.startswith(p) for p in ['file://', 'https://', 's3://', 'hdfs://', 'gs://', 'abfs://', 'wasb://']):
            # No recognized protocol - assume it's a local path
            if os.path.isabs(warehouse_root):
                warehouse_uri = f"file://{warehouse_root}"
            else:
                warehouse_uri = f"file://{os.path.abspath(warehouse_root)}"
    else:
        # Default to file:// with absolute path
        warehouse_uri = f"file://{os.path.abspath(warehouse_dir)}"
    
    # Note: For non-file:// schemes, we still create a local catalog
    # but the metadata will reference the custom warehouse_uri
    catalog = SqlCatalog(
        "ar_io_catalog",
        **{
            "uri": f"sqlite:///{catalog_path}",
            "warehouse": warehouse_uri,
        }
    )
    
    # Ensure namespace exists
    existing_namespaces = catalog.list_namespaces()
    if ("default",) not in existing_namespaces and "default" not in existing_namespaces:
        try:
            catalog.create_namespace("default")
            print("  Created namespace: default")
        except Exception as e:
            print(f"  Warning: Could not create namespace: {e}")
    
    return catalog


def register_parquet_files_as_iceberg(catalog, table_name, schema, parquet_files, warehouse_dir):
    """Register existing Parquet files as an Iceberg table without copying data"""
    
    # Create or replace the table
    namespace = "default"
    table_identifier = f"{namespace}.{table_name}"
    
    try:
        # Try to drop existing table
        catalog.drop_table(table_identifier)
        print(f"  Dropped existing table: {table_identifier}")
    except Exception:
        pass  # Table might not exist
    
    # Create partition spec based on height field
    # Note: We use IdentityTransform to keep the height values as-is
    partition_spec = PartitionSpec(
        PartitionField(
            source_id=2 if table_name == "blocks" else (12 if table_name == "transactions" else 1),  # height field ID
            field_id=1000,
            transform=IdentityTransform(),
            name="height"
        )
    )
    
    # Create the table with schema and partition spec
    table = catalog.create_table(
        identifier=table_identifier,
        schema=schema,
        partition_spec=partition_spec,
        properties={
            "write.format.default": "parquet",
            "write.parquet.compression-codec": "zstd",
            "read.split.metadata-target-size": "134217728",  # 128MB
        }
    )
    print(f"  Created Iceberg table: {table_identifier}")
    
    # Now we need to add the Parquet files to the table
    # PyIceberg doesn't have a direct add_files API like Java Iceberg,
    # so we'll read and append the data
    print(f"  Processing {len(parquet_files)} Parquet files...")
    
    for i, parquet_file in enumerate(parquet_files, 1):
        try:
            # Read the Parquet file directly without schema inference
            parquet_file_abs = os.path.abspath(parquet_file)
            
            # Use PyArrow Dataset API for better handling
            dataset = ds.dataset(parquet_file_abs, format="parquet")
            arrow_table = dataset.to_table()
            
            # Convert types to match Iceberg schema
            # This is necessary because PyArrow and Iceberg have slightly different type systems
            converted_data = []
            schema_fields = arrow_table.schema
            
            for batch in arrow_table.to_batches():
                arrays = []
                for field_idx, field in enumerate(schema_fields):
                    arr = batch.column(field_idx)
                    
                    # Convert unsigned to signed integers
                    if pa.types.is_unsigned_integer(field.type):
                        if pa.types.is_uint64(field.type):
                            arr = arr.cast(pa.int64())
                        elif pa.types.is_uint32(field.type):
                            arr = arr.cast(pa.int32())
                        elif pa.types.is_uint16(field.type) or pa.types.is_uint8(field.type):
                            arr = arr.cast(pa.int32())
                    
                    arrays.append(arr)
                
                converted_batch = pa.RecordBatch.from_arrays(arrays, schema=arrow_table.schema)
                converted_data.append(converted_batch)
            
            converted_table = pa.Table.from_batches(converted_data)
            
            # Append to the Iceberg table
            table.append(converted_table)
            
            if i % 10 == 0 or i == len(parquet_files):
                print(f"    Processed {i}/{len(parquet_files)} files")
                
        except Exception as e:
            print(f"    Warning: Failed to process {parquet_file}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"  Successfully registered {len(parquet_files)} Parquet files")
    
    # Create version-hint.text for DuckDB compatibility
    metadata_dir = os.path.join(warehouse_dir, "default.db", table_name, "metadata")
    version_hint_file = os.path.join(metadata_dir, "version-hint.text")
    
    # Find the latest metadata file version
    metadata_files = glob.glob(os.path.join(metadata_dir, "*.metadata.json"))
    if metadata_files:
        # Extract version numbers from filenames
        versions = []
        for mf in metadata_files:
            basename = os.path.basename(mf)
            # Format is 00000-uuid.metadata.json, 00001-uuid.metadata.json, etc.
            if "-" in basename and basename.endswith(".metadata.json"):
                version_str = basename.split("-")[0]
                try:
                    versions.append(int(version_str))
                except ValueError:
                    continue
        
        if versions:
            latest_version = max(versions)
            # Write version without newline for DuckDB compatibility
            with open(version_hint_file, "wb") as f:
                f.write(str(latest_version).encode())
            print(f"  Created version-hint.text with version: {latest_version}")
            
            # Also create v{version}.metadata.json symlink for DuckDB
            latest_metadata_file = None
            for mf in metadata_files:
                basename = os.path.basename(mf)
                if basename.startswith(f"{latest_version:05d}-"):
                    latest_metadata_file = basename
                    break
            
            if latest_metadata_file:
                symlink_name = os.path.join(metadata_dir, f"v{latest_version}.metadata.json")
                if os.path.exists(symlink_name):
                    os.remove(symlink_name)
                os.symlink(latest_metadata_file, symlink_name)
                print(f"  Created symlink: v{latest_version}.metadata.json -> {latest_metadata_file}")
    
    return table


def main():
    parser = argparse.ArgumentParser(
        description='Generate Apache Iceberg metadata for AR.IO Parquet exports (Fixed Version)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
This version correctly matches the schema that DuckDB exports to Parquet.

Examples:
  %(prog)s --warehouse-dir data/parquet-test
  %(prog)s --warehouse-dir /path/to/warehouse
  %(prog)s --warehouse-dir data/export --warehouse-root https://example.com/warehouse
  %(prog)s --warehouse-dir /local/data --warehouse-root s3://my-bucket/iceberg
  
Query the generated tables:
  Python with PyIceberg:
    from pyiceberg.catalog.sql import SqlCatalog
    catalog = SqlCatalog('catalog', uri='sqlite:///path/to/warehouse/catalog.db', 
                         warehouse='file:///path/to/warehouse')
    table = catalog.load_table('default.blocks')
    df = table.scan().to_pandas()
    
  DuckDB with Iceberg extension:
    INSTALL iceberg;
    LOAD iceberg;
    SELECT * FROM iceberg_scan('path/to/warehouse/default.db/blocks');
"""
    )
    
    parser.add_argument(
        '--warehouse-dir',
        default='data/local/warehouse',
        help='Warehouse directory containing Parquet data (default: data/local/warehouse)'
    )
    
    parser.add_argument(
        '--warehouse-root',
        help='Optional warehouse root URI for metadata references (e.g., file:///path, https://example.com/warehouse, s3://bucket/prefix). '
             'Note: For remote URIs, metadata is still written locally but references will point to the remote location. '
             'If not provided, defaults to file:// with absolute path to warehouse-dir'
    )
    
    args = parser.parse_args()
    
    # Check if warehouse directory exists
    if not os.path.exists(args.warehouse_dir):
        print(f"Error: Warehouse directory does not exist: {args.warehouse_dir}", file=sys.stderr)
        sys.exit(1)
    
    print(f"Generating Iceberg metadata (Fixed Version)")
    print(f"Warehouse: {args.warehouse_dir}")
    print()
    
    # Create catalog
    try:
        catalog = create_local_catalog(args.warehouse_dir, args.warehouse_root)
        if args.warehouse_root:
            print(f"Created/opened Iceberg catalog at: {args.warehouse_dir}/catalog.db")
            print(f"Metadata will reference warehouse root: {args.warehouse_root}")
            if not args.warehouse_root.startswith('file://'):
                print(f"Note: Catalog and metadata are written locally, but table locations reference remote URI")
        else:
            print(f"Created/opened Iceberg catalog at: {args.warehouse_dir}/catalog.db")
    except Exception as e:
        print(f"Error creating catalog: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # Process each table
    tables = [
        ("blocks", get_blocks_schema()),
        ("transactions", get_transactions_schema()),
        ("tags", get_tags_schema()),
    ]
    
    successful_tables = []
    
    for table_name, schema in tables:
        table_dir = os.path.join(args.warehouse_dir, table_name)
        if not os.path.exists(table_dir):
            print(f"Skipping {table_name}: directory does not exist")
            continue
        
        print(f"\nProcessing table: {table_name}")
        
        # Find all Parquet files
        data_dir = os.path.join(table_dir, "data")
        if not os.path.exists(data_dir):
            print(f"  No data directory found for {table_name}")
            continue
        
        parquet_files = glob.glob(os.path.join(data_dir, "*/*.parquet"))
        if not parquet_files:
            print(f"  No Parquet files found for {table_name}")
            continue
        
        print(f"  Found {len(parquet_files)} Parquet files")
        
        try:
            # Register Parquet files as Iceberg table
            table = register_parquet_files_as_iceberg(
                catalog, 
                table_name, 
                schema, 
                parquet_files,
                args.warehouse_dir
            )
            print(f"  Table location: {table.location()}")
            successful_tables.append(table_name)
        except Exception as e:
            print(f"  Error creating table {table_name}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if successful_tables:
        print()
        print("=" * 60)
        print("Iceberg metadata generation complete!")
        print(f"Successfully created {len(successful_tables)} tables: {', '.join(successful_tables)}")
        print(f"Catalog location: {args.warehouse_dir}/catalog.db")
        print()
        print("To query these tables:")
        print()
        print("1. Python with PyIceberg:")
        print("-" * 40)
        print("from pyiceberg.catalog.sql import SqlCatalog")
        print(f"catalog = SqlCatalog('catalog', uri='sqlite:///{os.path.abspath(args.warehouse_dir)}/catalog.db',")
        print(f"                     warehouse='file://{os.path.abspath(args.warehouse_dir)}')")
        print("table = catalog.load_table('default.blocks')")
        print("df = table.scan().to_pandas()")
        print("print(df.head())")
        print()
        print("2. DuckDB with Iceberg extension:")
        print("-" * 40)
        print("duckdb")
        print("INSTALL avro;  -- Required for Iceberg")
        print("INSTALL iceberg;")
        print("LOAD iceberg;")
        print(f"SELECT COUNT(*) FROM iceberg_scan('{os.path.abspath(args.warehouse_dir)}/default.db/blocks');")
    else:
        print("\nNo tables were successfully created.")
        sys.exit(1)


if __name__ == '__main__':
    main()