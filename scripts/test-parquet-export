#!/usr/bin/env bash

# Test script for new Parquet export with Iceberg metadata generation
# This demonstrates the new features without Dagster

set -euo pipefail

echo "========================================="
echo "AR.IO Parquet Export Test (No Dagster)"
echo "========================================="
echo ""

# Configuration
TEST_START_HEIGHT=1000000
TEST_END_HEIGHT=1001000  # Small range for testing
HEIGHT_PARTITION_SIZE=100  # Smaller partitions for testing
MAX_FILE_ROWS=50000
WAREHOUSE_DIR="data/local/warehouse"

echo "Test Configuration:"
echo "  Height range: $TEST_START_HEIGHT - $TEST_END_HEIGHT"
echo "  Partition size: $HEIGHT_PARTITION_SIZE blocks"
echo "  Max rows per file: $MAX_FILE_ROWS"
echo "  Warehouse dir: $WAREHOUSE_DIR"
echo ""

# Step 1: Clean up any previous test data
echo "Step 1: Cleaning up previous test data..."
rm -rf data/staging/job-* 2>/dev/null || true
rm -rf "$WAREHOUSE_DIR"/{blocks,transactions,tags} 2>/dev/null || true
echo "  Cleanup complete"
echo ""

# Step 2: Run the new Parquet export script
echo "Step 2: Running Parquet export with partitioning..."
echo "  Command: ./scripts/parquet-export-v2 \\"
echo "    --startHeight $TEST_START_HEIGHT \\"
echo "    --endHeight $TEST_END_HEIGHT \\"
echo "    --heightPartitionSize $HEIGHT_PARTITION_SIZE \\"
echo "    --maxFileRows $MAX_FILE_ROWS \\"
echo "    --includeL1Transactions \\"
echo "    --includeL1Tags"
echo ""

if [[ -f ./scripts/parquet-export-v2 ]]; then
  time ./scripts/parquet-export-v2 \
    --startHeight $TEST_START_HEIGHT \
    --endHeight $TEST_END_HEIGHT \
    --heightPartitionSize $HEIGHT_PARTITION_SIZE \
    --maxFileRows $MAX_FILE_ROWS \
    --includeL1Transactions \
    --includeL1Tags
  
  export_status=$?
  if [[ $export_status -ne 0 ]]; then
    echo "  ERROR: Export failed with status $export_status"
    exit 1
  fi
  echo "  Export completed successfully"
else
  echo "  WARNING: parquet-export-v2 script not found, skipping export"
  echo "  This is expected if databases are not available"
fi
echo ""

# Step 3: Generate Iceberg metadata
echo "Step 3: Generating Iceberg metadata..."
echo "  Command: python3 ./scripts/generate-iceberg-metadata.py \\"
echo "    --warehouse-dir $WAREHOUSE_DIR \\"
echo "    --partition-size $HEIGHT_PARTITION_SIZE"
echo ""

if [[ -f ./scripts/generate-iceberg-metadata.py ]]; then
  python3 ./scripts/generate-iceberg-metadata.py \
    --warehouse-dir $WAREHOUSE_DIR \
    --partition-size $HEIGHT_PARTITION_SIZE
  
  metadata_status=$?
  if [[ $metadata_status -ne 0 ]]; then
    echo "  ERROR: Metadata generation failed with status $metadata_status"
    exit 1
  fi
  echo "  Metadata generation completed successfully"
else
  echo "  WARNING: generate-iceberg-metadata.py script not found"
fi
echo ""

# Step 4: Verify the output structure
echo "Step 4: Verifying warehouse structure..."
echo ""
echo "Directory structure created:"
if [[ -d "$WAREHOUSE_DIR" ]]; then
  tree -L 3 "$WAREHOUSE_DIR" 2>/dev/null || find "$WAREHOUSE_DIR" -type d | head -20
else
  echo "  Warehouse directory not found (expected if export was skipped)"
fi
echo ""

# Step 5: Show sample queries
echo "Step 5: Sample queries for the exported data:"
echo ""
echo "DuckDB queries:"
echo "  # Install Iceberg extension"
echo "  INSTALL iceberg;"
echo "  LOAD iceberg;"
echo ""
echo "  # Query blocks table"
echo "  SELECT * FROM iceberg_scan('$WAREHOUSE_DIR/blocks/metadata/metadata.json') LIMIT 10;"
echo ""
echo "  # Query transactions by height range"
echo "  SELECT height, COUNT(*) as tx_count"
echo "  FROM iceberg_scan('$WAREHOUSE_DIR/transactions/metadata/metadata.json')"
echo "  WHERE height BETWEEN $TEST_START_HEIGHT AND $TEST_END_HEIGHT"
echo "  GROUP BY height"
echo "  ORDER BY height;"
echo ""
echo "  # Query tags with specific name"
echo "  SELECT * FROM iceberg_scan('$WAREHOUSE_DIR/tags/metadata/metadata.json')"
echo "  WHERE tag_name = 'App-Name'"
echo "  LIMIT 10;"
echo ""

# Step 6: Test crash recovery
echo "Step 6: Testing crash recovery feature..."
echo ""
echo "To test crash recovery:"
echo "  1. Start an export: ./scripts/parquet-export-v2 --startHeight 1000000 --endHeight 1010000"
echo "  2. Interrupt with Ctrl+C after a few partitions"
echo "  3. Resume with: ./scripts/parquet-export-v2 --startHeight 1000000 --endHeight 1010000 --resume"
echo "  4. Export will continue from the last checkpoint"
echo ""

echo "========================================="
echo "Test completed!"
echo "========================================="
echo ""
echo "Key improvements implemented:"
echo "  ✓ Configurable height-based partitioning (HEIGHT_PARTITION_SIZE)"
echo "  ✓ Crash recovery with checkpointing (--resume flag)"
echo "  ✓ Atomic operations (staging → warehouse)"
echo "  ✓ Iceberg-compatible directory structure"
echo "  ✓ Metadata generation for analytics tools"
echo "  ✓ No Dagster dependency required"
echo ""
echo "The system is now ready for:"
echo "  - DuckDB queries with Iceberg extension"
echo "  - Apache Spark with Iceberg catalog"
echo "  - Future ClickHouse integration (optional)"
echo "  - Other Iceberg-compatible tools"