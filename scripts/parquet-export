#!/usr/bin/env bash

# AR.IO Gateway - Production Parquet Export Script
# Features: Iceberg-compatible partitioning, data integrity verification, resume capability,
# performance timing, and comprehensive error handling

# Load common library
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/lib/common.sh"

set -euo pipefail

# Check required commands
require_commands duckdb || exit 1

# Override log_timing with millisecond precision version for this script
log_timing() {
  log_timing_ms "$1" "$2" "${3:-$(date +%s%N)}" "${SHOW_TIMING:-false}"
}

usage() {
  cat <<USAGE
Usage: $0 --output-dir DIR --start-height N --end-height N [options]

Options:
  --height-partition-size N Number of blocks per partition (default: 1000)
  --max-file-rows N         Max rows per file within partition (default: 1000000)
  --include-l1-transactions Include L1 transactions (default is to skip)
  --include-l1-tags         Include L1 transaction tags (default is to skip)
  --core-db PATH            Path to core SQLite database (default: data/sqlite/core.db)
  --bundles-db PATH         Path to bundles SQLite database (default: data/sqlite/bundles.db)
  --staging-dir PATH        Staging directory for temp files (default: data/etl/staging)
  --staging-job-dir PATH    Explicit path for job staging directory (overrides auto-generation)
  --dataset-dir PATH        Final dataset directory (default: data/datasets/default)
  --skip-dataset-move       Skip moving files from staging to dataset directory
  --resume                  Resume from last checkpoint if export was interrupted
  --verify-count            Verify row counts match SQLite after each partition (slower)
  --show-timing             Show detailed timing information for operations
  
  Iceberg Metadata:
  --generate-iceberg        Generate Apache Iceberg metadata after export (requires PyIceberg)
  --iceberg-root <URI>      Warehouse root URI for Iceberg metadata (e.g., s3://bucket/path, https://host/path)

Note: This creates Iceberg-compatible directory structure with partitioned Parquet files.
      Count verification ensures data integrity but adds processing time.
USAGE
}

# Configuration defaults
OUTPUT_DIR=""
START_HEIGHT=""
END_HEIGHT=""
HEIGHT_PARTITION_SIZE=1000
MAX_FILE_ROWS=1000000
SKIP_L1_TRANSACTIONS=true
SKIP_L1_TAGS=true
CORE_DB_PATH="data/sqlite/core.db"
BUNDLES_DB_PATH="data/sqlite/bundles.db"
STAGING_DIR="${ETL_STAGING_PATH:-data/etl/staging}"
DATASET_DIR="${LOCAL_DATASETS_PATH:-data/datasets}/default"
RESUME=false
VERIFY_COUNT=false
RUN_ID="$(date +%Y%m%d_%H%M%S)_$$"
EXPLICIT_JOB_DIR=false
SKIP_DATASET_MOVE=false

# Load environment configuration
load_env


# Iceberg metadata generation flag
GENERATE_ICEBERG=false
ICEBERG_ROOT=""

# Parse arguments
while [[ $# -gt 0 ]]; do
  case $1 in
    --output-dir)
      OUTPUT_DIR=$2
      shift 2
      ;;
    --start-height)
      START_HEIGHT=$2
      shift 2
      ;;
    --end-height)
      END_HEIGHT=$2
      shift 2
      ;;
    --height-partition-size)
      HEIGHT_PARTITION_SIZE=$2
      shift 2
      ;;
    --max-file-rows)
      MAX_FILE_ROWS=$2
      shift 2
      ;;
    --include-l1-transactions)
      SKIP_L1_TRANSACTIONS=false
      shift 1
      ;;
    --include-l1-tags)
      SKIP_L1_TAGS=false
      shift 1
      ;;
    --core-db)
      CORE_DB_PATH=$2
      shift 2
      ;;
    --bundles-db)
      BUNDLES_DB_PATH=$2
      shift 2
      ;;
    --staging-dir)
      STAGING_DIR=$2
      shift 2
      ;;
    --dataset-dir)
      DATASET_DIR=$2
      shift 2
      ;;
    --resume)
      RESUME=true
      shift 1
      ;;
    --verify-count)
      VERIFY_COUNT=true
      shift 1
      ;;
    --show-timing)
      SHOW_TIMING=true
      shift 1
      ;;
    --generate-iceberg)
      GENERATE_ICEBERG=true
      shift 1
      ;;
    --staging-job-dir)
      JOB_STAGING_DIR=$2
      EXPLICIT_JOB_DIR=true
      shift 2
      ;;
    --skip-dataset-move)
      SKIP_DATASET_MOVE=true
      shift 1
      ;;
    --iceberg-root)
      ICEBERG_ROOT="$2"
      shift 2
      ;;
    -h|--help)
      usage
      exit 0
      ;;
    *)
      echo "Unknown option: $1" >&2
      usage
      exit 1
      ;;
  esac
done

# If outputDir not specified, use warehouse structure
if [[ -z "$OUTPUT_DIR" ]]; then
  if [[ -z "$START_HEIGHT" || -z "$END_HEIGHT" ]]; then
    echo "Error: Must specify --start-height and --end-height" >&2
    usage
    exit 1
  fi
  # Use dataset directory with proper structure
  OUTPUT_DIR="$DATASET_DIR"
fi

# Validate required arguments
if [[ -z "$START_HEIGHT" || -z "$END_HEIGHT" ]]; then
  echo "Error: Must specify --start-height and --end-height" >&2
  usage
  exit 1
fi

# Validate numeric arguments to prevent SQL injection
for v in "$START_HEIGHT" "$END_HEIGHT" "$HEIGHT_PARTITION_SIZE" "$MAX_FILE_ROWS"; do
  [[ $v =~ ^[0-9]+$ ]] || { echo "Error: Non-numeric argument detected: $v" >&2; exit 1; }
done

# Validate numeric bounds
if (( START_HEIGHT > END_HEIGHT )); then
  echo "Error: startHeight ($START_HEIGHT) cannot be greater than endHeight ($END_HEIGHT)" >&2
  exit 1
fi

# Setup directories
if [ "$EXPLICIT_JOB_DIR" = false ]; then
  JOB_STAGING_DIR="$STAGING_DIR/job-$RUN_ID"
fi
CHECKPOINT_FILE="$JOB_STAGING_DIR/.checkpoint"
VERIFICATION_LOG="$JOB_STAGING_DIR/.verification_results"

# Cleanup function
cleanup() {
  local exit_code=$?
  if [[ $exit_code -ne 0 ]]; then
    if [[ -d "$JOB_STAGING_DIR" ]]; then
      echo "Export failed. Temporary files preserved in: $JOB_STAGING_DIR"
      echo "Run with --resume to continue from checkpoint"
    fi
  else
    # Clean up on successful completion
    if [ "$SKIP_DATASET_MOVE" = false ]; then
      rm -rf "$JOB_STAGING_DIR"
    fi
    rm -f "$TEMP_DB" "$TEMP_DB.wal" "$SQL_INIT"
  fi
}
trap cleanup EXIT

# Create necessary directories
mkdir -p "$JOB_STAGING_DIR"
mkdir -p "$OUTPUT_DIR"/{blocks,transactions,tags}/{data,metadata}


if $VERIFY_COUNT; then
  echo "Count verification enabled - will verify data integrity after each partition"
fi

# Check for resume
LAST_PROCESSED_HEIGHT=$((START_HEIGHT - 1))
if $RESUME; then
  # Find existing checkpoint
  for dir in "$STAGING_DIR"/job-*; do
    if [[ -f "$dir/.checkpoint" ]]; then
      checkpoint_height=$(cat "$dir/.checkpoint")
      # Check if this checkpoint is within our range
      if [[ $checkpoint_height -ge $START_HEIGHT && $checkpoint_height -lt $END_HEIGHT ]]; then
        LAST_PROCESSED_HEIGHT=$checkpoint_height
        JOB_STAGING_DIR="$dir"
        CHECKPOINT_FILE="$dir/.checkpoint"
        VERIFICATION_LOG="$dir/.verification_results"
        echo "Found checkpoint at height: $checkpoint_height"
        echo "Resuming from height: $((LAST_PROCESSED_HEIGHT + 1))"
        break
      fi
    fi
  done
  
  if [[ $LAST_PROCESSED_HEIGHT -eq $((START_HEIGHT - 1)) ]]; then
    echo "No valid checkpoint found for range $START_HEIGHT-$END_HEIGHT"
    echo "Starting fresh export"
  fi
fi

# Setup temporary database
TEMP_DB="$JOB_STAGING_DIR/export.duckdb"
SQL_INIT="$JOB_STAGING_DIR/init.sql"

# Function to save checkpoint
save_checkpoint() {
  echo "$1" > "$CHECKPOINT_FILE"
}


# Function to verify counts between SQLite and DuckDB
verify_partition_counts() {
  local partition_start=$1
  local partition_end=$2
  local partition_dir="height=${partition_start}-${partition_end}"
  
  if ! $VERIFY_COUNT; then
    return 0
  fi
  
  echo "  Verifying data counts for partition: $partition_dir"
  
  local verification_passed=true
  
  # Verify blocks count
  local sqlite_blocks=$(duckdb "$TEMP_DB" -csv -noheader -c "
    ATTACH '${CORE_DB_PATH}' AS core (TYPE SQLITE, READONLY TRUE);
    SELECT COUNT(*) FROM core.stable_blocks 
    WHERE height BETWEEN $partition_start AND $partition_end;
  ")
  
  local duckdb_blocks=$(duckdb "$TEMP_DB" -csv -noheader -c "
    SELECT COUNT(*) FROM blocks 
    WHERE height >= $partition_start AND height <= $partition_end;
  ")
  
  if [[ "$sqlite_blocks" != "$duckdb_blocks" ]]; then
    echo "    ERROR: Block count mismatch! SQLite: $sqlite_blocks, DuckDB: $duckdb_blocks" >&2
    verification_passed=false
  else
    echo "    ✓ Blocks: $duckdb_blocks (matches SQLite)"
  fi
  
  # Verify L2 transactions (data items) count
  local sqlite_l2_tx=$(duckdb "$TEMP_DB" -csv -noheader -c "
    ATTACH '${BUNDLES_DB_PATH}' AS bundles (TYPE SQLITE, READONLY TRUE);
    SELECT COUNT(*) FROM bundles.stable_data_items 
    WHERE height BETWEEN $partition_start AND $partition_end;
  ")
  
  local duckdb_l2_tx=$(duckdb "$TEMP_DB" -csv -noheader -c "
    SELECT COUNT(*) FROM transactions 
    WHERE is_data_item = 1 
    AND height >= $partition_start AND height <= $partition_end;
  ")
  
  if [[ "$sqlite_l2_tx" != "$duckdb_l2_tx" ]]; then
    echo "    ERROR: L2 transaction count mismatch! SQLite: $sqlite_l2_tx, DuckDB: $duckdb_l2_tx" >&2
    verification_passed=false
  else
    echo "    ✓ L2 Transactions: $duckdb_l2_tx (matches SQLite)"
  fi
  
  # Verify L1 transactions if included
  if ! $SKIP_L1_TRANSACTIONS; then
    local sqlite_l1_tx=$(duckdb "$TEMP_DB" -csv -noheader -c "
      ATTACH '${CORE_DB_PATH}' AS core (TYPE SQLITE, READONLY TRUE);
      SELECT COUNT(*) FROM core.stable_transactions 
      WHERE height BETWEEN $partition_start AND $partition_end;
    ")
    
    local duckdb_l1_tx=$(duckdb "$TEMP_DB" -csv -noheader -c "
      SELECT COUNT(*) FROM transactions 
      WHERE is_data_item = 0 
      AND height >= $partition_start AND height <= $partition_end;
    ")
    
    if [[ "$sqlite_l1_tx" != "$duckdb_l1_tx" ]]; then
      echo "    ERROR: L1 transaction count mismatch! SQLite: $sqlite_l1_tx, DuckDB: $duckdb_l1_tx" >&2
      verification_passed=false
    else
      echo "    ✓ L1 Transactions: $duckdb_l1_tx (matches SQLite)"
    fi
  fi
  
  # Verify L2 tags count
  local sqlite_l2_tags=$(duckdb "$TEMP_DB" -csv -noheader -c "
    ATTACH '${BUNDLES_DB_PATH}' AS bundles (TYPE SQLITE, READONLY TRUE);
    SELECT COUNT(*) FROM bundles.stable_data_item_tags 
    WHERE height BETWEEN $partition_start AND $partition_end;
  ")
  
  local duckdb_l2_tags=$(duckdb "$TEMP_DB" -csv -noheader -c "
    SELECT COUNT(*) FROM tags 
    WHERE is_data_item = 1 
    AND height >= $partition_start AND height <= $partition_end;
  ")
  
  if [[ "$sqlite_l2_tags" != "$duckdb_l2_tags" ]]; then
    echo "    ERROR: L2 tags count mismatch! SQLite: $sqlite_l2_tags, DuckDB: $duckdb_l2_tags" >&2
    verification_passed=false
  else
    echo "    ✓ L2 Tags: $duckdb_l2_tags (matches SQLite)"
  fi
  
  # Verify L1 tags if included
  if ! $SKIP_L1_TAGS && ! $SKIP_L1_TRANSACTIONS; then
    local sqlite_l1_tags=$(duckdb "$TEMP_DB" -csv -noheader -c "
      ATTACH '${CORE_DB_PATH}' AS core (TYPE SQLITE, READONLY TRUE);
      SELECT COUNT(*) FROM core.stable_transaction_tags 
      WHERE height BETWEEN $partition_start AND $partition_end;
    ")
    
    local duckdb_l1_tags=$(duckdb "$TEMP_DB" -csv -noheader -c "
      SELECT COUNT(*) FROM tags 
      WHERE is_data_item = 0 
      AND height >= $partition_start AND height <= $partition_end;
    ")
    
    if [[ "$sqlite_l1_tags" != "$duckdb_l1_tags" ]]; then
      echo "    ERROR: L1 tags count mismatch! SQLite: $sqlite_l1_tags, DuckDB: $duckdb_l1_tags" >&2
      verification_passed=false
    else
      echo "    ✓ L1 Tags: $duckdb_l1_tags (matches SQLite)"
    fi
  fi
  
  # Verify exported Parquet files
  if $verification_passed; then
    echo "  Verifying exported Parquet files..."
    
    # Check blocks parquet
    if ls "$JOB_STAGING_DIR/blocks/$partition_dir"/*.parquet 1>/dev/null 2>&1; then
      local parquet_blocks=$(duckdb -csv -noheader -c "
        SELECT COUNT(*) FROM read_parquet('$JOB_STAGING_DIR/blocks/$partition_dir/*.parquet', hive_partitioning=false);
      ")
      
      if [[ "$parquet_blocks" != "$duckdb_blocks" ]]; then
        echo "    ERROR: Blocks Parquet count mismatch! Expected: $duckdb_blocks, Found: $parquet_blocks" >&2
        verification_passed=false
      else
        echo "    ✓ Blocks Parquet: $parquet_blocks rows"
      fi
      
      # Verify block uniqueness
      echo "  Verifying block uniqueness..."
      
      # Check block hash uniqueness
      local hash_dups=$(duckdb -csv -noheader -c "
        WITH hash_counts AS (
          SELECT hash, COUNT(*) as cnt
          FROM read_parquet('$JOB_STAGING_DIR/blocks/$partition_dir/*.parquet', hive_partitioning=false)
          WHERE hash IS NOT NULL
          GROUP BY hash
          HAVING COUNT(*) > 1
        )
        SELECT COUNT(*) FROM hash_counts;
      " 2>/dev/null || echo "error")
      
      if [[ "$hash_dups" == "error" ]]; then
        echo "    ⚠ Could not verify block hash uniqueness"
      elif [[ "$hash_dups" == "0" ]] || [[ -z "$hash_dups" ]]; then
        echo "    ✓ All block hashes are unique"
      else
        echo "    ✗ Found $hash_dups duplicate block hashes"
        verification_passed=false
      fi
      
      # Check indep_hash uniqueness
      local indep_dups=$(duckdb -csv -noheader -c "
        WITH hash_counts AS (
          SELECT indep_hash, COUNT(*) as cnt
          FROM read_parquet('$JOB_STAGING_DIR/blocks/$partition_dir/*.parquet', hive_partitioning=false)
          WHERE indep_hash IS NOT NULL
          GROUP BY indep_hash
          HAVING COUNT(*) > 1
        )
        SELECT COUNT(*) FROM hash_counts;
      " 2>/dev/null || echo "error")
      
      if [[ "$indep_dups" == "error" ]]; then
        echo "    ⚠ Could not verify indep_hash uniqueness"
      elif [[ "$indep_dups" == "0" ]] || [[ -z "$indep_dups" ]]; then
        echo "    ✓ All block indep_hashes are unique"
      else
        echo "    ✗ Found $indep_dups duplicate indep_hashes"
        verification_passed=false
      fi
    else
      echo "    WARNING: No blocks Parquet files found!" >&2
      verification_passed=false
    fi
    
    # Check transactions parquet
    local expected_tx_count=$(duckdb "$TEMP_DB" -csv -noheader -c "
      SELECT COUNT(*) FROM transactions 
      WHERE height >= $partition_start AND height <= $partition_end;
    ")
    
    # Check if any parquet files exist
    if ls "$JOB_STAGING_DIR/transactions/$partition_dir"/*.parquet 1>/dev/null 2>&1; then
      local parquet_tx=$(duckdb -csv -noheader -c "
        SELECT COUNT(*) FROM read_parquet('$JOB_STAGING_DIR/transactions/$partition_dir/*.parquet', hive_partitioning=false);
      ")
      
      if [[ "$parquet_tx" != "$expected_tx_count" ]]; then
        echo "    ERROR: Transactions Parquet count mismatch! Expected: $expected_tx_count, Found: $parquet_tx" >&2
        verification_passed=false
      else
        echo "    ✓ Transactions Parquet: $parquet_tx rows"
      fi
      
      # Verify transaction ID uniqueness if there are transactions
      if [[ "$parquet_tx" != "0" ]]; then
        echo "  Verifying transaction uniqueness..."
        local id_dups=$(duckdb -csv -noheader -c "
          WITH id_counts AS (
            SELECT id, COUNT(*) as cnt
            FROM read_parquet('$JOB_STAGING_DIR/transactions/$partition_dir/*.parquet', hive_partitioning=false)
            WHERE id IS NOT NULL
            GROUP BY id
            HAVING COUNT(*) > 1
          )
          SELECT COUNT(*) FROM id_counts;
        " 2>/dev/null || echo "error")
        
        if [[ "$id_dups" == "error" ]]; then
          echo "    ⚠ Could not verify transaction ID uniqueness"
        elif [[ "$id_dups" == "0" ]] || [[ -z "$id_dups" ]]; then
          echo "    ✓ All transaction IDs are unique"
        else
          echo "    ✗ Found $id_dups duplicate transaction IDs"
          verification_passed=false
        fi
      fi
    else
      # No files means 0 rows (which is valid if expected_tx_count is also 0)
      local parquet_tx=0
      if [[ "$parquet_tx" != "$expected_tx_count" ]]; then
        echo "    ERROR: Transactions Parquet count mismatch! Expected: $expected_tx_count, Found: $parquet_tx" >&2
        verification_passed=false
      else
        echo "    ✓ Transactions Parquet: $parquet_tx rows"
      fi
    fi
    
    # Check tags parquet
    local expected_tags_count=$(duckdb "$TEMP_DB" -csv -noheader -c "
      SELECT COUNT(*) FROM tags 
      WHERE height >= $partition_start AND height <= $partition_end;
    ")
    
    # Check if any parquet files exist
    if ls "$JOB_STAGING_DIR/tags/$partition_dir"/*.parquet 1>/dev/null 2>&1; then
      local parquet_tags=$(duckdb -csv -noheader -c "
        SELECT COUNT(*) FROM read_parquet('$JOB_STAGING_DIR/tags/$partition_dir/*.parquet', hive_partitioning=false);
      ")
    else
      # No files means 0 rows (which is valid if expected_tags_count is also 0)
      local parquet_tags=0
    fi
    
    if [[ "$parquet_tags" != "$expected_tags_count" ]]; then
      echo "    ERROR: Tags Parquet count mismatch! Expected: $expected_tags_count, Found: $parquet_tags" >&2
      verification_passed=false
    else
      echo "    ✓ Tags Parquet: $parquet_tags rows"
    fi
  fi
  
  # Log verification results
  echo "$(date -Iseconds)|$partition_dir|$verification_passed" >> "$VERIFICATION_LOG"
  
  if ! $verification_passed; then
    echo "    ERROR: Verification failed for partition $partition_dir!" >&2
    echo "    Check the logs above for details." >&2
    return 1
  fi
  
  echo "    ✓ All verification checks passed"
  return 0
}


# Initialize database once at the start
initialize_database() {
  # Check if SQLite databases exist
  if [[ ! -f "$CORE_DB_PATH" ]]; then
    echo "Error: Core database not found at: $CORE_DB_PATH" >&2
    exit 1
  fi
  
  if [[ ! -f "$BUNDLES_DB_PATH" ]]; then
    echo "Error: Bundles database not found at: $BUNDLES_DB_PATH" >&2
    exit 1
  fi
  
  if [[ ! -f "$TEMP_DB" ]]; then
    echo "Initializing DuckDB database..."
    
    # First create the database with schema
    if [[ ! -f "src/database/duckdb/schema.sql" ]]; then
      echo "Error: DuckDB schema file not found at src/database/duckdb/schema.sql" >&2
      exit 1
    fi
    
    echo "Creating DuckDB schema..."
    duckdb "$TEMP_DB" < src/database/duckdb/schema.sql
    
    # Then install SQLite extension and attach databases
    echo "Attaching SQLite databases..."
    echo "  Core DB: $CORE_DB_PATH"
    echo "  Bundles DB: $BUNDLES_DB_PATH"
    
    duckdb "$TEMP_DB" <<SQL
INSTALL sqlite;
LOAD sqlite;
ATTACH '${CORE_DB_PATH}' AS core (TYPE SQLITE, READONLY TRUE);
ATTACH '${BUNDLES_DB_PATH}' AS bundles (TYPE SQLITE, READONLY TRUE);

-- Verify attachments by checking table existence (much faster than COUNT)
SELECT 'Verifying core database...';
SELECT 1 FROM core.stable_blocks LIMIT 1;
SELECT 'Verifying bundles database...';
SELECT 1 FROM bundles.stable_data_items LIMIT 1;
SQL
    
    if [[ $? -ne 0 ]]; then
      echo "Error: Failed to initialize DuckDB or attach SQLite databases" >&2
      exit 1
    fi
  else
    echo "Reusing existing DuckDB database, re-attaching SQLite databases..."
    duckdb "$TEMP_DB" <<SQL
INSTALL sqlite;
LOAD sqlite;
ATTACH '${CORE_DB_PATH}' AS core (TYPE SQLITE, READONLY TRUE);
ATTACH '${BUNDLES_DB_PATH}' AS bundles (TYPE SQLITE, READONLY TRUE);

-- Verify attachments by checking table existence (much faster than COUNT)
SELECT 'Verifying core database...';
SELECT 1 FROM core.stable_blocks LIMIT 1;
SELECT 'Verifying bundles database...';
SELECT 1 FROM bundles.stable_data_items LIMIT 1;
SQL
    
    if [[ $? -ne 0 ]]; then
      echo "Error: Failed to re-attach SQLite databases" >&2
      exit 1
    fi
  fi
}

# Function to import data for a height range
import_height_range() {
  local start_h=$1
  local end_h=$2
  
  echo "Importing data for height range: $start_h to $end_h"
  
  local import_start=$(date +%s%N)
  
  # Build and execute import SQL
  {
    # Re-attach databases for this session
    echo "ATTACH '${CORE_DB_PATH}' AS core (TYPE SQLITE, READONLY TRUE);"
    echo "ATTACH '${BUNDLES_DB_PATH}' AS bundles (TYPE SQLITE, READONLY TRUE);"
    
    # Import blocks for this range
    local blocks_start=$(date +%s%N)
    echo "-- Importing blocks"
    echo "INSERT INTO blocks"
    echo "SELECT indep_hash, height, previous_block, nonce, hash, block_timestamp, tx_count, block_size"
    echo "FROM core.stable_blocks"
    echo "WHERE height BETWEEN ${start_h} AND ${end_h};"
    
    # Import L1 transactions if enabled
    if ! $SKIP_L1_TRANSACTIONS; then
      echo "INSERT INTO transactions"
      echo "SELECT"
      echo "  st.id, NULL AS indexed_at, st.block_transaction_index,"
      echo "  0 AS is_data_item,"
      echo "  st.target, st.quantity, st.reward,"
      echo "  st.last_tx AS anchor,"  # L1 uses last_tx as anchor
      echo "  st.data_size, st.content_type, st.format,"
      echo "  st.height, st.owner_address, st.data_root,"
      echo "  NULL AS parent,"
      echo "  st.\"offset\","  # Need quotes as offset is a reserved word
      echo "  NULL AS size,"  # L1 transactions don't have size field
      echo "  NULL AS data_offset,"  # L1 doesn't have data_offset
      echo "  NULL AS owner_offset,"  # L1 doesn't have owner_offset  
      echo "  NULL AS owner_size,"    # L1 doesn't have owner_size
      echo "  CASE"
      echo "    WHEN octet_length(w.public_modulus) <= 64 THEN w.public_modulus"
      echo "    ELSE NULL"
      echo "  END AS owner,"
      echo "  NULL AS signature_offset,"  # L1 doesn't have signature_offset
      echo "  NULL AS signature_size,"    # L1 doesn't have signature_size
      echo "  NULL AS signature_type,"    # L1 doesn't have signature_type
      echo "  st.id AS root_transaction_id,"
      echo "  NULL AS root_parent_offset"
      echo "FROM core.stable_transactions st"
      echo "LEFT JOIN core.wallets w ON st.owner_address = w.address"
      echo "WHERE st.height BETWEEN ${start_h} AND ${end_h};"
      
      # Import L1 tags if enabled
      if ! $SKIP_L1_TAGS; then
        echo "INSERT INTO tags"
        echo "SELECT"
        echo "  st.height,"
        echo "  st.id,"
        echo "  stt.transaction_tag_index AS tag_index,"
        echo "  NULL AS indexed_at,"
        echo "  tn.name AS tag_name,"
        echo "  tv.value AS tag_value,"
        echo "  0 AS is_data_item"
        echo "FROM core.stable_transactions st"
        echo "CROSS JOIN core.stable_transaction_tags stt"
        echo "CROSS JOIN core.tag_names tn"
        echo "CROSS JOIN core.tag_values tv"
        echo "WHERE st.id = stt.transaction_id"
        echo "  AND stt.tag_name_hash = tn.hash"
        echo "  AND stt.tag_value_hash = tv.hash"
        echo "  AND st.height BETWEEN ${start_h} AND ${end_h};"
      fi
    fi
    
    # Import L2 transactions (data items)
    echo "INSERT INTO transactions"
    echo "SELECT"
    echo "  sdi.id, sdi.indexed_at, sdi.block_transaction_index,"
    echo "  1 AS is_data_item,"
    echo "  sdi.target, NULL AS quantity, NULL AS reward, sdi.anchor, sdi.data_size, sdi.content_type,"
    echo "  NULL AS format,"
    echo "  sdi.height, sdi.owner_address,"
    echo "  NULL AS data_root,"  # L2 data items don't have data_root
    echo "  sdi.parent_id AS parent,"
    echo "  sdi.\"offset\", sdi.size, sdi.data_offset,"  # Quote offset as it's a reserved word
    echo "  sdi.owner_offset, sdi.owner_size,"
    echo "  CASE"
    echo "    WHEN octet_length(w.public_modulus) <= 64 THEN w.public_modulus"
    echo "    ELSE NULL"
    echo "  END AS owner,"
    echo "  sdi.signature_offset, sdi.signature_size, sdi.signature_type,"
    echo "  sdi.root_transaction_id,"
    echo "  sdi.root_parent_offset"
    echo "FROM bundles.stable_data_items sdi"
    echo "LEFT JOIN bundles.wallets w ON sdi.owner_address = w.address"
    echo "WHERE sdi.height BETWEEN ${start_h} AND ${end_h};"
    
    # Import L2 tags
    echo "INSERT INTO tags"
    echo "SELECT"
    echo "  sdi.height,"
    echo "  sdi.id,"
    echo "  sdit.data_item_tag_index AS tag_index,"
    echo "  sdi.indexed_at,"
    echo "  tn.name AS tag_name,"
    echo "  tv.value AS tag_value,"
    echo "  1 AS is_data_item"
    echo "FROM bundles.stable_data_items sdi"
    echo "CROSS JOIN bundles.stable_data_item_tags sdit"
    echo "CROSS JOIN bundles.tag_names tn"
    echo "CROSS JOIN bundles.tag_values tv"
    echo "WHERE sdi.id = sdit.data_item_id"
    echo "  AND sdit.tag_name_hash = tn.hash"
    echo "  AND sdit.tag_value_hash = tv.hash"
    echo "  AND sdi.height BETWEEN ${start_h} AND ${end_h};"
    
  } | duckdb "$TEMP_DB"
  
  if [[ $? -ne 0 ]]; then
    echo "Error: Failed to import data for height range $start_h to $end_h" >&2
    return 1
  fi
  
  local import_end=$(date +%s%N)
  log_timing "Total import for range $start_h-$end_h" "$import_start" "$import_end"
  
  # Count what was imported for timing analysis
  if [[ "${SHOW_TIMING:-false}" == "true" ]]; then
    local block_count=$(duckdb "$TEMP_DB" -csv -noheader -c "SELECT COUNT(*) FROM blocks WHERE height >= $start_h AND height <= $end_h;")
    local tx_count=$(duckdb "$TEMP_DB" -csv -noheader -c "SELECT COUNT(*) FROM transactions WHERE height >= $start_h AND height <= $end_h;")
    local tag_count=$(duckdb "$TEMP_DB" -csv -noheader -c "SELECT COUNT(*) FROM tags WHERE height >= $start_h AND height <= $end_h;")
    echo "  [TIMING] Imported: $block_count blocks, $tx_count transactions, $tag_count tags" >&2
  fi
}

# Function to export partition data
export_partition_data() {
  local partition_start=$1
  local partition_end=$2
  local partition_dir="height=${partition_start}-${partition_end}"
  
  echo "Exporting partition: $partition_dir"
  echo "  Staging dir: $JOB_STAGING_DIR"
  
  local export_start=$(date +%s%N)
  
  # Create staging directories for this partition
  mkdir -p "$JOB_STAGING_DIR"/{blocks,transactions,tags}/"$partition_dir"
  
  # Export blocks
  local blocks_export_start=$(date +%s%N)
  local block_count=$(duckdb "$TEMP_DB" -csv -noheader -c "SELECT COUNT(*) FROM blocks WHERE height >= $partition_start AND height <= $partition_end;")
  if [[ $block_count -gt 0 ]]; then
    local block_file="$JOB_STAGING_DIR/blocks/$partition_dir/blocks_${partition_start}_${partition_end}_${RUN_ID}.parquet"
    duckdb "$TEMP_DB" -c "COPY (SELECT * FROM blocks WHERE height >= $partition_start AND height <= $partition_end ORDER BY height, indep_hash) TO '$block_file' (FORMAT PARQUET, COMPRESSION 'zstd');"
  fi
  log_timing "Export blocks ($block_count rows)" "$blocks_export_start"
  
  # Export transactions with file splitting if needed
  local tx_export_start=$(date +%s%N)
  local tx_count=$(duckdb "$TEMP_DB" -csv -noheader -c "SELECT COUNT(*) FROM transactions WHERE height >= $partition_start AND height <= $partition_end;")
  if [[ $tx_count -gt 0 ]]; then
    if [[ $tx_count -le $MAX_FILE_ROWS ]]; then
      # Single file
      local tx_file="$JOB_STAGING_DIR/transactions/$partition_dir/transactions_${partition_start}_${partition_end}_${RUN_ID}.parquet"
      duckdb "$TEMP_DB" -c "COPY (SELECT * FROM transactions WHERE height >= $partition_start AND height <= $partition_end ORDER BY height, id) TO '$tx_file' (FORMAT PARQUET, COMPRESSION 'zstd');"
    else
      # Multiple files needed
      local file_num=0
      local offset=0
      while [[ $offset -lt $tx_count ]]; do
        local tx_file="$JOB_STAGING_DIR/transactions/$partition_dir/transactions_${partition_start}_${partition_end}_${file_num}_${RUN_ID}.parquet"
        duckdb "$TEMP_DB" -c "COPY (SELECT * FROM transactions WHERE height >= $partition_start AND height <= $partition_end ORDER BY height, id LIMIT $MAX_FILE_ROWS OFFSET $offset) TO '$tx_file' (FORMAT PARQUET, COMPRESSION 'zstd');"
        offset=$((offset + MAX_FILE_ROWS))
        file_num=$((file_num + 1))
      done
    fi
  fi
  log_timing "Export transactions ($tx_count rows)" "$tx_export_start"
  
  # Export tags with file splitting if needed
  local tags_export_start=$(date +%s%N)
  local tag_count=$(duckdb "$TEMP_DB" -csv -noheader -c "SELECT COUNT(*) FROM tags WHERE height >= $partition_start AND height <= $partition_end;")
  if [[ $tag_count -gt 0 ]]; then
    if [[ $tag_count -le $MAX_FILE_ROWS ]]; then
      # Single file
      local tag_file="$JOB_STAGING_DIR/tags/$partition_dir/tags_${partition_start}_${partition_end}_${RUN_ID}.parquet"
      duckdb "$TEMP_DB" -c "COPY (SELECT * FROM tags WHERE height >= $partition_start AND height <= $partition_end ORDER BY height, id, tag_index) TO '$tag_file' (FORMAT PARQUET, COMPRESSION 'zstd');"
    else
      # Multiple files needed
      local file_num=0
      local offset=0
      while [[ $offset -lt $tag_count ]]; do
        local tag_file="$JOB_STAGING_DIR/tags/$partition_dir/tags_${partition_start}_${partition_end}_${file_num}_${RUN_ID}.parquet"
        duckdb "$TEMP_DB" -c "COPY (SELECT * FROM tags WHERE height >= $partition_start AND height <= $partition_end ORDER BY height, id, tag_index LIMIT $MAX_FILE_ROWS OFFSET $offset) TO '$tag_file' (FORMAT PARQUET, COMPRESSION 'zstd');"
        offset=$((offset + MAX_FILE_ROWS))
        file_num=$((file_num + 1))
      done
    fi
  fi
  log_timing "Export tags ($tag_count rows)" "$tags_export_start"
  
  echo "  Exported: $block_count blocks, $tx_count transactions, $tag_count tags"
  
  local export_end=$(date +%s%N)
  log_timing "Total export for partition $partition_dir" "$export_start" "$export_end"
}

# Initialize database
initialize_database

# Process height range in partitions
echo "Starting export from height $((LAST_PROCESSED_HEIGHT + 1)) to $END_HEIGHT"
echo "Partition size: $HEIGHT_PARTITION_SIZE blocks"
echo "Max file rows: $MAX_FILE_ROWS"

current_height=$((LAST_PROCESSED_HEIGHT + 1))

while [[ $current_height -le $END_HEIGHT ]]; do
  # Calculate partition boundaries
  current_partition_start=$current_height
  current_partition_end=$((current_height + HEIGHT_PARTITION_SIZE - 1))
  
  # Don't exceed the requested end height
  if [[ $current_partition_end -gt $END_HEIGHT ]]; then
    current_partition_end=$END_HEIGHT
  fi
  
  # Import data for this partition
  import_height_range $current_partition_start $current_partition_end
  
  # Export partition data to staging
  export_partition_data $current_partition_start $current_partition_end
  
  # Verify counts if enabled (after export)
  if $VERIFY_COUNT; then
    if ! verify_partition_counts $current_partition_start $current_partition_end; then
      echo "ERROR: Verification failed. Stopping export." >&2
      exit 1
    fi
  fi
  
  # Verify exported Parquet files if count verification is enabled
  partition_dir="height=${current_partition_start}-${current_partition_end}"
  
  
  # Move to dataset directory (atomic operation) unless --skip-dataset-move is set
  if [ "$SKIP_DATASET_MOVE" = false ]; then
    for table in blocks transactions tags; do
      if [[ -d "$JOB_STAGING_DIR/$table/$partition_dir" ]]; then
        mkdir -p "$OUTPUT_DIR/$table/data/$partition_dir"
        mv "$JOB_STAGING_DIR/$table/$partition_dir"/* "$OUTPUT_DIR/$table/data/$partition_dir/" 2>/dev/null || true
        rmdir "$JOB_STAGING_DIR/$table/$partition_dir" 2>/dev/null || true
      fi
    done
  fi
  
  echo "Completed partition: $partition_dir"
  
  # Clear imported data from temporary database to save space
  duckdb "$TEMP_DB" <<SQL
DELETE FROM blocks WHERE height >= $current_partition_start AND height <= $current_partition_end;
DELETE FROM transactions WHERE height >= $current_partition_start AND height <= $current_partition_end;
DELETE FROM tags WHERE height >= $current_partition_start AND height <= $current_partition_end;
SQL
  
  # Update checkpoint
  save_checkpoint $current_partition_end
  
  # Move to next partition
  current_height=$((current_partition_end + 1))
done

echo "Export completed successfully!"
echo "Output directory: $OUTPUT_DIR"


if $VERIFY_COUNT && [[ -f "$VERIFICATION_LOG" ]]; then
  echo "Verification log: $VERIFICATION_LOG"
fi

# Generate Iceberg metadata if requested
if [ "$GENERATE_ICEBERG" = true ]; then
  echo "Generating Apache Iceberg metadata..."
  if command -v python3 >/dev/null 2>&1; then
    if python3 -c "import pyiceberg" 2>/dev/null; then
      # Build the command with optional warehouse root
      ICEBERG_CMD="python3 \"$SCRIPT_DIR/generate-iceberg-metadata\" --warehouse-dir \"$OUTPUT_DIR\""
      if [ -n "$ICEBERG_ROOT" ]; then
        ICEBERG_CMD="$ICEBERG_CMD --warehouse-root \"$ICEBERG_ROOT\""
        echo "  Using warehouse root: $ICEBERG_ROOT"
      fi
      
      if eval $ICEBERG_CMD; then
        echo "  ✓ Iceberg metadata generated successfully"
      else
        echo "  ⚠ Warning: Iceberg metadata generation failed, but export data is intact"
      fi
    else
      echo "  ⚠ Warning: PyIceberg not installed. Run: pip install 'pyiceberg[pyarrow,duckdb,sql]'"
    fi
  else
    echo "  ⚠ Warning: Python3 not available, cannot generate Iceberg metadata"
  fi
fi

# Clean up successful job (handled by cleanup trap)
exit 0