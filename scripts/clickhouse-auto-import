#!/usr/bin/env bash

# AR.IO Gateway - Automated ClickHouse Import Pipeline
# This script continuously exports data from AR.IO SQLite databases to Parquet format
# with Iceberg-compatible structure and imports it to ClickHouse.
# 
# Uses the parquet-export script for both export and ClickHouse import in one operation.
# The exported Parquet files are stored with Iceberg-compatible partitioning in the warehouse directory.

# Load common library
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/lib/common.sh"

set -euo pipefail

# Load environment and API key
load_env
load_admin_api_key
load_clickhouse_config

# Set local variables with defaults
ar_io_host=${AR_IO_HOST:-localhost}
ar_io_port=${AR_IO_PORT:-4000}
warehouse_dir=${CLICKHOUSE_WAREHOUSE_DIR:-./data/warehouse}
sleep_interval=${CLICKHOUSE_AUTO_IMPORT_SLEEP_INTERVAL:-3600} # Export every hour by default
height_interval=${CLICKHOUSE_AUTO_IMPORT_HEIGHT_INTERVAL:-10000}
max_rows_per_file=${CLICKHOUSE_AUTO_IMPORT_MAX_ROWS_PER_FILE:-1000000}

# Check critical preconditions
if [ -z "${ADMIN_API_KEY:-}" ]; then
    echo "Error: Either no ADMIN_API_KEY environment variable set in .env or no ADMIN_API_KEY_FILE path provided"
    exit 1
fi

# Check that parquet-export script exists
if [ ! -x "./scripts/parquet-export" ]; then
    echo "Error: ./scripts/parquet-export not found or not executable"
    exit 1
fi

# Create directories
processed_dir="$warehouse_dir/.processed"
mkdir -p "$warehouse_dir" "$processed_dir"

# ------------------------------------------------------------------------------
# SWITCH OFF 'exit on error' so the script doesn't die in the main loop
# ------------------------------------------------------------------------------
set +e

while true; do
    loop_start_time=$(date +%s)
    echo "Starting new import cycle at $(date)"

    echo "Attempting to fetch debug info from admin endpoint..."
    fetch_debug_start_time=$(date +%s)
    debug_info="$(curl -s --fail --show-error -H "Authorization: Bearer $ADMIN_API_KEY" "http://${ar_io_host}:${ar_io_port}/ar-io/admin/debug")"

    # If curl fails or returns non-200, skip this iteration
    fetch_debug_end_time=$(date +%s)
    if [ $? -ne 0 ] || [ -z "$debug_info" ]; then
        log_timing "Fetch debug info (failed)" "$fetch_debug_start_time" "$fetch_debug_end_time"
        echo "Warning: Failed to get debug info or received empty response. Skipping this iteration."
        sleep 10
        continue
    fi
    log_timing "Fetch debug info" "$fetch_debug_start_time" "$fetch_debug_end_time"

    # Parse debug info
    min_height="$(echo "$debug_info" | jq -r '.db.heights.minStableDataItem' 2>/dev/null)"
    max_height="$(echo "$debug_info" | jq -r '.db.heights.maxStableDataItem' 2>/dev/null)"
    max_indexed_at="$(echo "$debug_info" | jq -r '.db.timestamps.maxStableDataItemIndexedAt' 2>/dev/null)"

    # If jq parsing failed or the fields are empty, skip this iteration
    if [ -z "$min_height" ] || [ -z "$max_height" ] || [ -z "$max_indexed_at" ] || [ "$min_height" = "null" ] || [ "$max_height" = "null" ]; then
        echo "Warning: Debug info missing expected fields or invalid JSON. Skipping this iteration."
        sleep 10
        continue
    fi

    # Align to intervals of 10,000
    current_height=$(((min_height / height_interval) * height_interval))

    # Inner loop for heights
    height_process_start_time=$(date +%s)
    while [ "$current_height" -le "$max_height" ]; do
        batch_start_time=$(date +%s)
        echo "-------------------------------------------"
        echo "Starting batch at $(date)"
        end_height=$((current_height + height_interval))

        echo "Processing heights $current_height to $end_height..."

        # Step 1: Export to Parquet using parquet-export script with ClickHouse integration
        echo "Starting Parquet export with ClickHouse integration..."
        export_start_time=$(date +%s)
        
        # Run parquet-export script with ClickHouse enabled
        # The script handles both export and import in one operation
        ./scripts/parquet-export \
            --startHeight "$current_height" \
            --endHeight "$end_height" \
            --heightPartitionSize "$height_interval" \
            --outputDir "$warehouse_dir" \
            --maxFileRows "$max_rows_per_file" \
            --includeL1Transactions \
            --includeL1Tags \
            --includeL2Transactions \
            --includeL2Tags \
            --enableClickhouse \
            --clickhouseHost "$CLICKHOUSE_HOST" \
            --clickhousePort "$CLICKHOUSE_PORT" \
            --clickhouseUser "$CLICKHOUSE_USER" \
            --clickhousePassword "$CLICKHOUSE_PASSWORD"
        
        export_status=$?
        export_complete_time=$(date +%s)
        
        if [ $export_status -eq 0 ]; then
            echo "Export and ClickHouse import completed successfully."
            log_timing "Export to parquet and ClickHouse import" "$export_start_time" "$export_complete_time"
            import_success=true
        else
            echo "Warning: Export or ClickHouse import failed with status $export_status"
            log_timing "Export to parquet and ClickHouse import (failed)" "$export_start_time" "$export_complete_time"
            import_success=false
        fi

        # Step 2: Mark this height range as processed
        if [ "$import_success" = true ]; then
            touch "$processed_dir/heights_${current_height}_${end_height}.done"
        fi

        # Step 3: Prune stable data items only if import was successful
        if [ "$import_success" = true ]; then
            echo "Pruning stable data items from height $current_height to $end_height..."
            prune_start_time=$(date +%s)
            curl -s --fail --show-error -X POST "http://${ar_io_host}:${ar_io_port}/ar-io/admin/prune-stable-data-items" \
                -H "Authorization: Bearer $ADMIN_API_KEY" \
                -H "Content-Type: application/json" \
                -d "{
                    \"indexedAtThreshold\": $max_indexed_at,
                    \"startHeight\": $current_height,
                    \"endHeight\": $end_height
                }"
            prune_end_time=$(date +%s)
            if [ $? -ne 0 ]; then
                log_timing "Prune stable data items (failed)" "$prune_start_time" "$prune_end_time"
                echo "Warning: Prune request failed. Continuing to next batch."
            else
                log_timing "Prune stable data items" "$prune_start_time" "$prune_end_time"
            fi
        else
            echo "Skipping pruning because import failed."
        fi

        # Log batch timing
        batch_end_time=$(date +%s)
        log_timing "Batch processing (heights $current_height to $end_height)" "$batch_start_time" "$batch_end_time"

        # Bump current_height
        current_height=$end_height
    done

    # Log timing for the heights processing
    height_process_end_time=$(date +%s)
    log_timing "All heights processing" "$height_process_start_time" "$height_process_end_time"

    # Log timing for the entire cycle
    loop_end_time=$(date +%s)
    log_timing "Complete import cycle" "$loop_start_time" "$loop_end_time"

    echo "Sleeping for $sleep_interval seconds..."
    sleep "$sleep_interval"
done
