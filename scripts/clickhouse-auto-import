#!/usr/bin/env bash

# AR.IO Gateway - Automated ClickHouse Import Pipeline
# This script continuously exports data from AR.IO SQLite databases to Parquet format
# with Iceberg-compatible structure and imports it to ClickHouse.
# 
# Uses a two-step approach:
# 1. parquet-export script to export data to Parquet files with Iceberg-compatible partitioning
# 2. clickhouse-import script to import the Parquet files to ClickHouse database
# 
# The exported Parquet files are stored with Iceberg-compatible partitioning in the warehouse directory.

# Load common library
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/lib/common.sh"

set -euo pipefail

# Load environment and API key
load_env
load_admin_api_key
load_clickhouse_config

# Set local variables with defaults
ar_io_host=${AR_IO_HOST:-localhost}
ar_io_port=${AR_IO_PORT:-4000}
dataset_dir="${LOCAL_DATASETS_PATH:-./data/datasets}/default"
staging_dir="${ETL_STAGING_PATH:-./data/etl/staging}"
sleep_interval=${CLICKHOUSE_AUTO_IMPORT_SLEEP_INTERVAL:-3600} # Export every hour by default
height_interval=${CLICKHOUSE_AUTO_IMPORT_HEIGHT_INTERVAL:-10000}
max_rows_per_file=${CLICKHOUSE_AUTO_IMPORT_MAX_ROWS_PER_FILE:-1000000}

# Check critical preconditions
if [ -z "${ADMIN_API_KEY:-}" ]; then
    echo "Error: Either no ADMIN_API_KEY environment variable set in .env or no ADMIN_API_KEY_FILE path provided"
    exit 1
fi

# Check that parquet-export script exists
if [ ! -x "./scripts/parquet-export" ]; then
    echo "Error: ./scripts/parquet-export not found or not executable"
    exit 1
fi

# Create directories
processed_dir="$dataset_dir/.processed"
mkdir -p "$dataset_dir" "$processed_dir"

# ------------------------------------------------------------------------------
# SWITCH OFF 'exit on error' so the script doesn't die in the main loop
# ------------------------------------------------------------------------------
set +e

while true; do
    loop_start_time=$(date +%s)
    echo "Starting new import cycle at $(date)"

    echo "Attempting to fetch debug info from admin endpoint..."
    fetch_debug_start_time=$(date +%s)
    debug_info="$(curl -s --fail --show-error -H "Authorization: Bearer $ADMIN_API_KEY" "http://${ar_io_host}:${ar_io_port}/ar-io/admin/debug")"

    # If curl fails or returns non-200, skip this iteration
    fetch_debug_end_time=$(date +%s)
    if [ $? -ne 0 ] || [ -z "$debug_info" ]; then
        log_timing "Fetch debug info (failed)" "$fetch_debug_start_time" "$fetch_debug_end_time"
        echo "Warning: Failed to get debug info or received empty response. Skipping this iteration."
        sleep 10
        continue
    fi
    log_timing "Fetch debug info" "$fetch_debug_start_time" "$fetch_debug_end_time"

    # Parse debug info
    min_height="$(echo "$debug_info" | jq -r '.db.heights.minStableDataItem' 2>/dev/null)"
    max_height="$(echo "$debug_info" | jq -r '.db.heights.maxStableDataItem' 2>/dev/null)"
    max_indexed_at="$(echo "$debug_info" | jq -r '.db.timestamps.maxStableDataItemIndexedAt' 2>/dev/null)"

    # If jq parsing failed or the fields are empty, skip this iteration
    if [ -z "$min_height" ] || [ -z "$max_height" ] || [ -z "$max_indexed_at" ] || [ "$min_height" = "null" ] || [ "$max_height" = "null" ]; then
        echo "Warning: Debug info missing expected fields or invalid JSON. Skipping this iteration."
        sleep 10
        continue
    fi

    # Align to intervals of 10,000
    current_height=$(((min_height / height_interval) * height_interval))

    # Inner loop for heights
    height_process_start_time=$(date +%s)
    while [ "$current_height" -le "$max_height" ]; do
        batch_start_time=$(date +%s)
        echo "-------------------------------------------"
        echo "Starting batch at $(date)"
        end_height=$((current_height + height_interval))

        echo "Processing heights $current_height to $end_height..."

        # Generate job staging directory for this batch
        job_id="$(date +%Y%m%d_%H%M%S)_$$"
        job_staging_dir="$staging_dir/job-${current_height}-${end_height}-${job_id}"
        
        # Step 1: Export to Parquet format in staging
        echo "Starting Parquet export..."
        export_start_time=$(date +%s)
        
        # Run parquet-export script with staging-based workflow
        # Note: L2 transactions (data items) are included by default, only L1 needs explicit inclusion
        ./scripts/parquet-export \
            --start-height "$current_height" \
            --end-height "$end_height" \
            --height-partition-size "$height_interval" \
            --staging-job-dir "$job_staging_dir" \
            --output-dir "$dataset_dir" \
            --skip-dataset-move \
            --max-file-rows "$max_rows_per_file"
        
        export_status=$?
        export_complete_time=$(date +%s)
        
        if [ $export_status -eq 0 ]; then
            echo "Parquet export completed successfully."
            log_timing "Export to parquet" "$export_start_time" "$export_complete_time"
            export_success=true
        else
            echo "Warning: Parquet export failed with status $export_status"
            log_timing "Export to parquet (failed)" "$export_start_time" "$export_complete_time"
            export_success=false
        fi

        # Step 2: Import to ClickHouse from staging if export succeeded
        if [ "$export_success" = true ]; then
            echo "Starting ClickHouse import from staging..."
            import_start_time=$(date +%s)
            
            # Generate partition name matching the height range format
            partition_name="height=${current_height}-$((end_height - 1))"
            
            import_args=(--input-dir "$job_staging_dir" --partition "$partition_name")
            [ -n "${CLICKHOUSE_HOST:-}" ]     && import_args+=(--clickhouse-host "$CLICKHOUSE_HOST")
            [ -n "${CLICKHOUSE_PORT:-}" ]     && import_args+=(--clickhouse-port "$CLICKHOUSE_PORT")
            [ -n "${CLICKHOUSE_USER:-}" ]     && import_args+=(--clickhouse-user "$CLICKHOUSE_USER")
            [ -n "${CLICKHOUSE_PASSWORD:-}" ] && import_args+=(--clickhouse-password "$CLICKHOUSE_PASSWORD")
            ./scripts/clickhouse-import "${import_args[@]}"
            
            import_status=$?
            import_complete_time=$(date +%s)
            
            if [ $import_status -eq 0 ]; then
                echo "ClickHouse import completed successfully."
                log_timing "ClickHouse import" "$import_start_time" "$import_complete_time"
                import_success=true
                
                # Step 3: Move from staging to dataset directory after successful import
                echo "Moving files from staging to dataset directory..."
                move_start_time=$(date +%s)
                
                for table in blocks transactions tags; do
                    if [ -d "$job_staging_dir/$table" ]; then
                        mkdir -p "$dataset_dir/$table/data"
                        mv "$job_staging_dir/$table"/* "$dataset_dir/$table/data/" 2>/dev/null || true
                    fi
                done
                
                move_complete_time=$(date +%s)
                log_timing "Move staging to dataset" "$move_start_time" "$move_complete_time"
                
                # Step 3.5: Update Iceberg metadata for new partitions
                if command -v python3 >/dev/null 2>&1 && [ -f ./scripts/generate-iceberg-metadata ]; then
                    echo "Updating Iceberg metadata..."
                    iceberg_start_time=$(date +%s)
                    
                    # Build warehouse root based on ARNS_ROOT_HOST if configured
                    iceberg_args=(
                        "--warehouse-dir" "$dataset_dir"
                    )
                    
                    # Add ARNS warehouse root if configured
                    if [ -n "${ARNS_ROOT_HOST:-}" ]; then
                        warehouse_root="https://${ARNS_ROOT_HOST}/local/datasets/default"
                        iceberg_args+=("--warehouse-root" "$warehouse_root")
                        echo "  Using ARNS warehouse root: $warehouse_root"
                    fi
                    
                    # Run the metadata generation
                    if python3 ./scripts/generate-iceberg-metadata "${iceberg_args[@]}"; then
                        iceberg_status=0
                    else
                        iceberg_status=1
                    fi
                    
                    iceberg_complete_time=$(date +%s)
                    
                    if [ $iceberg_status -eq 0 ]; then
                        echo "âœ“ Iceberg metadata updated successfully."
                        log_timing "Iceberg metadata generation" "$iceberg_start_time" "$iceberg_complete_time"
                    else
                        echo "Warning: Iceberg metadata generation failed (non-critical, continuing)"
                        log_timing "Iceberg metadata generation (failed)" "$iceberg_start_time" "$iceberg_complete_time"
                    fi
                else
                    echo "Skipping Iceberg metadata update (Python3 or script not available)"
                fi
                
                # Clean up staging directory after successful move
                rm -rf "$job_staging_dir"
                echo "Staging directory cleaned up."
                
            else
                echo "Warning: ClickHouse import failed with status $import_status"
                echo "Staging files preserved at: $job_staging_dir"
                log_timing "ClickHouse import (failed)" "$import_start_time" "$import_complete_time"
                import_success=false
            fi
        else
            echo "Skipping ClickHouse import because Parquet export failed."
            import_success=false
        fi

        # Step 4: Mark this height range as processed
        if [ "$import_success" = true ]; then
            touch "$processed_dir/heights_${current_height}_${end_height}.done"
        fi

        # Step 4: Prune stable data items only if import was successful
        if [ "$import_success" = true ]; then
            echo "Pruning stable data items from height $current_height to $end_height..."
            prune_start_time=$(date +%s)
            curl -s --fail --show-error -X POST "http://${ar_io_host}:${ar_io_port}/ar-io/admin/prune-stable-data-items" \
                -H "Authorization: Bearer $ADMIN_API_KEY" \
                -H "Content-Type: application/json" \
                -d "{
                    \"indexedAtThreshold\": $max_indexed_at,
                    \"startHeight\": $current_height,
                    \"endHeight\": $end_height
                }"
            prune_end_time=$(date +%s)
            if [ $? -ne 0 ]; then
                log_timing "Prune stable data items (failed)" "$prune_start_time" "$prune_end_time"
                echo "Warning: Prune request failed. Continuing to next batch."
            else
                log_timing "Prune stable data items" "$prune_start_time" "$prune_end_time"
            fi
        else
            echo "Skipping pruning because import failed."
        fi

        # Log batch timing
        batch_end_time=$(date +%s)
        log_timing "Batch processing (heights $current_height to $end_height)" "$batch_start_time" "$batch_end_time"

        # Bump current_height
        current_height=$end_height
    done

    # Log timing for the heights processing
    height_process_end_time=$(date +%s)
    log_timing "All heights processing" "$height_process_start_time" "$height_process_end_time"

    # Log timing for the entire cycle
    loop_end_time=$(date +%s)
    log_timing "Complete import cycle" "$loop_start_time" "$loop_end_time"

    echo "Sleeping for $sleep_interval seconds..."
    sleep "$sleep_interval"
done
