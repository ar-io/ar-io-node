#!/usr/bin/env bash

set -euo pipefail

# Function to log timing information
log_timing() {
    # Only log timing when DEBUG is set
    if [ -n "$DEBUG" ]; then
        local step_name=$1
        local start_time=$2
        local end_time=$3
        local duration_s=$(( end_time - start_time ))
        local duration_min=$(( duration_s / 60 ))
        local duration_sec=$(( duration_s % 60 ))

        echo "TIMING: $step_name - $duration_min min $duration_sec sec (total $duration_s seconds)"
    fi
}

# Export environment variables from .env if it exists
if [ -f .env ]; then
    set -a # automatically export all variables
    source .env
    set +a
fi

# If ADMIN_API_KEY_FILE is set, read its contents (trimming any trailing newline) into ADMIN_API_KEY
if [ -n "${ADMIN_API_KEY_FILE:-}" ]; then
    ADMIN_API_KEY="$(tr -d '\n' < "${ADMIN_API_KEY_FILE}")"
fi

# Set local variables with defaults
ar_io_host=${AR_IO_HOST:-localhost}
ar_io_port=${AR_IO_PORT:-4000}
parquet_dir=${PARQUET_DATA_PATH:-./data/parquet}
sleep_interval=${CLICKHOUSE_AUTO_IMPORT_SLEEP_INTERVAL:-3600} # Export every hour by default
height_interval=${CLICKHOUSE_AUTO_IMPORT_HEIGHT_INTERVAL:-10000}
max_rows_per_file=${CLICKHOUSE_AUTO_IMPORT_MAX_ROWS_PER_FILE:-1000000}

# Check critical preconditions
if [ -z "${ADMIN_API_KEY:-}" ]; then
    echo "Error: Either no ADMIN_API_KEY environment variable set in .env or no ADMIN_API_KEY_FILE path provided"
    exit 1
fi

# Create directories
imported_dir="$parquet_dir/imported"
mkdir -p "$parquet_dir" "$imported_dir"

# ------------------------------------------------------------------------------
# SWITCH OFF 'exit on error' so the script doesn't die in the main loop
# ------------------------------------------------------------------------------
set +e

while true; do
    loop_start_time=$(date +%s)
    echo "Starting new import cycle at $(date)"

    echo "Attempting to fetch debug info from admin endpoint..."
    fetch_debug_start_time=$(date +%s)
    debug_info="$(curl -s --fail --show-error -H "Authorization: Bearer $ADMIN_API_KEY" "http://${ar_io_host}:${ar_io_port}/ar-io/admin/debug")"

    # If curl fails or returns non-200, skip this iteration
    fetch_debug_end_time=$(date +%s)
    if [ $? -ne 0 ] || [ -z "$debug_info" ]; then
        log_timing "Fetch debug info (failed)" "$fetch_debug_start_time" "$fetch_debug_end_time"
        echo "Warning: Failed to get debug info or received empty response. Skipping this iteration."
        sleep 10
        continue
    fi
    log_timing "Fetch debug info" "$fetch_debug_start_time" "$fetch_debug_end_time"

    # Parse debug info
    min_height="$(echo "$debug_info" | jq -r '.db.heights.minStableDataItem' 2>/dev/null)"
    max_height="$(echo "$debug_info" | jq -r '.db.heights.maxStableDataItem' 2>/dev/null)"
    max_indexed_at="$(echo "$debug_info" | jq -r '.db.timestamps.maxStableDataItemIndexedAt' 2>/dev/null)"

    # If jq parsing failed or the fields are empty, skip this iteration
    if [ -z "$min_height" ] || [ -z "$max_height" ] || [ -z "$max_indexed_at" ] || [ "$min_height" = "null" ] || [ "$max_height" = "null" ]; then
        echo "Warning: Debug info missing expected fields or invalid JSON. Skipping this iteration."
        sleep 10
        continue
    fi

    # Align to intervals of 10,000
    current_height=$(((min_height / height_interval) * height_interval))

    # Inner loop for heights
    height_process_start_time=$(date +%s)
    while [ "$current_height" -le "$max_height" ]; do
        batch_start_time=$(date +%s)
        echo "-------------------------------------------"
        echo "Starting batch at $(date)"
        end_height=$((current_height + height_interval))

        echo "Processing heights $current_height to $end_height..."

        # Step 1: Export to Parquet
        echo "Sending export request..."
        export_start_time=$(date +%s)
        curl -s --fail --show-error -X POST "http://${ar_io_host}:${ar_io_port}/ar-io/admin/export-parquet" \
            -H "Authorization: Bearer $ADMIN_API_KEY" \
            -H "Content-Type: application/json" \
            -d "{
                \"outputDir\": \"$parquet_dir\",
                \"startHeight\": $current_height,
                \"endHeight\": $end_height,
                \"maxFileRows\": $max_rows_per_file
            }"

        # If curl failed, break out of the height loop and move on
        export_request_end_time=$(date +%s)
        if [ $? -ne 0 ]; then
            log_timing "Export request (failed)" "$export_start_time" "$export_request_end_time"
            echo "Warning: Export request failed, skipping this range."
            break
        fi
        log_timing "Export request sent" "$export_start_time" "$export_request_end_time"

        # Step 2: Wait for export to complete
        echo "Waiting for export to complete..."
        while true; do
            status="$(curl -s --fail --show-error -H "Authorization: Bearer $ADMIN_API_KEY" \
                "http://${ar_io_host}:${ar_io_port}/ar-io/admin/export-parquet/status")"

            if [ $? -ne 0 ] || [ -z "$status" ]; then
                echo "Warning: Failed to get export status. Cleaning partial files and moving on..."
                rm -f "$parquet_dir"/*.parquet
                break  # break out of waiting loop
            fi

            export_status="$(echo "$status" | jq -r '.status' 2>/dev/null)"
            if [ "$export_status" = "completed" ]; then
                echo "Export completed."
                break
            elif [ "$export_status" = "errored" ]; then
                error_msg="$(echo "$status" | jq -r '.error' 2>/dev/null)"
                echo "Warning: Export failed: $error_msg"
                rm -f "$parquet_dir"/*.parquet
                break
            fi

            echo "Export in progress. Waiting 10s..."
            sleep 10
        done

        export_complete_time=$(date +%s)
        if [ "$export_status" = "completed" ]; then
            log_timing "Export to parquet" "$export_start_time" "$export_complete_time"
        elif [ "$export_status" = "errored" ]; then
            log_timing "Export to parquet (failed)" "$export_start_time" "$export_complete_time"
        fi

        # Step 3: Attempt to import Parquet files
        import_success=false
        if compgen -G "$parquet_dir/*.parquet" > /dev/null; then
            echo "Importing Parquet files..."
            import_start_time=$(date +%s)
            if ./scripts/clickhouse-import; then
                import_success=true
                import_end_time=$(date +%s)
                log_timing "Clickhouse import" "$import_start_time" "$import_end_time"
            else
                import_end_time=$(date +%s)
                log_timing "Clickhouse import (failed)" "$import_start_time" "$import_end_time"
                echo "Warning: Clickhouse import failed. Moving on but preserving Parquet files in 'imported' folder..."
            fi

            # Step 4: Move processed files to imported directory
            mv "$parquet_dir"/*.parquet "$imported_dir/" 2>/dev/null
        else
            echo "No Parquet files to import for this batch."
        fi

        # Step 5: Prune stable data items only if import was successful
        if [ "$import_success" = true ]; then
            echo "Pruning stable data items from height $current_height to $end_height..."
            prune_start_time=$(date +%s)
            curl -s --fail --show-error -X POST "http://${ar_io_host}:${ar_io_port}/ar-io/admin/prune-stable-data-items" \
                -H "Authorization: Bearer $ADMIN_API_KEY" \
                -H "Content-Type: application/json" \
                -d "{
                    \"indexedAtThreshold\": $max_indexed_at,
                    \"startHeight\": $current_height,
                    \"endHeight\": $end_height
                }"
            prune_end_time=$(date +%s)
            if [ $? -ne 0 ]; then
                log_timing "Prune stable data items (failed)" "$prune_start_time" "$prune_end_time"
                echo "Warning: Prune request failed. Continuing to next batch."
            else
                log_timing "Prune stable data items" "$prune_start_time" "$prune_end_time"
            fi
        else
            echo "Skipping pruning because import failed."
        fi

        # Log batch timing
        batch_end_time=$(date +%s)
        log_timing "Batch processing (heights $current_height to $end_height)" "$batch_start_time" "$batch_end_time"

        # Bump current_height
        current_height=$end_height
    done

    # Log timing for the heights processing
    height_process_end_time=$(date +%s)
    log_timing "All heights processing" "$height_process_start_time" "$height_process_end_time"

    # Log timing for the entire cycle
    loop_end_time=$(date +%s)
    log_timing "Complete import cycle" "$loop_start_time" "$loop_end_time"

    echo "Sleeping for $sleep_interval seconds..."
    sleep "$sleep_interval"
done
